import os
import cv2
from utils import load_config
from torch.utils.data import Dataset, DataLoader, random_split
import torch



# ===============================================================
# AUGMENTATION PIPELINE
# ===============================================================
import albumentations as A
from albumentations.pytorch import ToTensorV2

IMG_SIZE = 64

train_transform = A.Compose([
    A.Resize(height=IMG_SIZE, width=IMG_SIZE),
    
    # --- 1. Chuy·ªÉn ƒë·ªïi H√¨nh h·ªçc (Geometric Transformations) ---
    A.HorizontalFlip(p=0.5), # L·∫≠t ngang 
    A.VerticalFlip(p=0.5),   # L·∫≠t d·ªçc
    A.RandomRotate90(p=0.5), # Xoay ng·∫´u nhi√™n 90 ƒë·ªô
    
    # --- 2. ƒêi·ªÅu ch·ªânh M√†u s·∫Øc & ƒê·ªô s√°ng (Color/Brightness Adjustments) ---
    A.OneOf([
        A.RandomBrightnessContrast(p=1), # TƒÉng/gi·∫£m ƒë·ªô s√°ng v√† ƒë·ªô t∆∞∆°ng ph·∫£n
        A.HueSaturationValue(p=1),       # Thay ƒë·ªïi m√†u s·∫Øc v√† ƒë·ªô b√£o h√≤a
    ], p=0.3), 
    
    # --- 3. Th√™m nhi·ªÖu (Noise Addition) ---
    A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.2),

    # --- 4. C·∫Øt/Che l·ªó (CoarseDropout - D√πng API c≈©) ---
    A.CoarseDropout(
        # ƒê√£ thay th·∫ø num_holes_range b·∫±ng max_holes
        max_holes=2, 
        # ƒê√£ thay th·∫ø hole_height_range b·∫±ng max_height
        max_height=20, 
        # ƒê√£ thay th·∫ø hole_width_range b·∫±ng max_width
        max_width=20, 
        fill_value=0, 
        p=0.2 
    ),
    
    # --- 5. B∆∞·ªõc cu·ªëi c√πng (Normalization & ToTensor) ---
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), 
    ToTensorV2(), 
])

# --- Val/Test ---
val_transform = A.Compose([
    A.Resize(height=IMG_SIZE, width=IMG_SIZE),
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ToTensorV2(),
])

class DatasetWrapper(Dataset):
    """
    Class n√†y nh·∫≠n v√†o m·ªôt t·∫≠p con (Subset) ch·ª©a ·∫£nh g·ªëc,
    v√† √°p d·ª•ng Augmentation l√™n n√≥.
    """
    def __init__(self, subset, transform=None):
        self.subset = subset
        self.transform = transform

    def __len__(self):
        return len(self.subset)

    def __getitem__(self, idx):
        # --- Take original image and label from subset ---
        image, label = self.subset[idx]
        
        # --- Apply transformation if any ---
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
            
        return image, label


# ===============================================================
# LOADING RAW DATASET
# ===============================================================
class GeoDataset(Dataset):
    def __init__(
        self, 
        data_config: dict,
    ):
        self.data_config = data_config
        self.raw_data_dir = data_config['RAW_DATA_DIR']
        self.labels = [] 
        
        raw_images_data, raw_images_path = self._load_raw_images_and_labels()
        
        dataset = {
            'image': [],
            'path': [],
            'class': [],
            'label_idx': []
        }

        for img_data, path in zip(raw_images_data, raw_images_path):
            class_name = os.path.basename(os.path.dirname(path))
            
            if class_name not in self.labels:
                self.labels.append(class_name)
            
            dataset['image'].append(img_data)
            dataset['path'].append(path)
            dataset['class'].append(class_name)

        self.class_to_idx = {cls: idx for idx, cls in enumerate(sorted(self.labels))}
        dataset['label_idx'] = [self.class_to_idx[cls] for cls in dataset['class']]
        self.dataset = dataset
        

    def _load_raw_images_and_labels(self):
        all_images_data = []
        all_images_path = []
        for class_name in os.listdir(self.raw_data_dir):
            class_dir = os.path.join(self.raw_data_dir, class_name)
            if os.path.isdir(class_dir):
                for file_name in os.listdir(class_dir):
                    if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                        img_path = os.path.join(class_dir, file_name)
                        img = cv2.imread(img_path)
                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                        
                        all_images_data.append(img)
                        all_images_path.append(img_path)

        return all_images_data, all_images_path
    
    def __len__(self):
        return len(self.dataset['image'])

    def __getitem__(self, idx):
        image = self.dataset['image'][idx]
        class_name = self.dataset['class'][idx]
        label = self.class_to_idx[class_name]
        return image, label
    
    @property
    def num_classes(self):
        return len(self.labels)
'''
data_config = load_config("config/data_config.yml")
dataset = GeoDataset(data_config=data_config)
print(f"Loaded dataset with {len(dataset.dataset['image'])} images.")
'''




# ===============================================================
# DATALOADER
# ===============================================================
def create_dataloaders(
    data_config, 
    train_transform = train_transform, 
    val_transform = val_transform, 
    split_ratio=(0.9, 0.05, 0.05), 
    batch_size=32
):
    # --- Load Full Dataset ---
    full_dataset = GeoDataset(data_config)
    
    # --- Calculate Split Sizes ---
    total_size = len(full_dataset)
    train_size = int(total_size * split_ratio[0])
    val_size = int(total_size * split_ratio[1])
    test_size = total_size - train_size - val_size
    
    print(f"üìä Chia t·∫≠p d·ªØ li·ªáu: Train={train_size}, Val={val_size}, Test={test_size}")
    
    # --- Split Dataset ---
    train_subset, val_subset, test_subset = random_split(
        full_dataset, 
        [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    # --- Apply Transformations ---
    train_dataset = DatasetWrapper(train_subset, transform=train_transform)
    val_dataset   = DatasetWrapper(val_subset,   transform=val_transform)
    test_dataset  = DatasetWrapper(test_subset,  transform=val_transform)
    
    # --- Create DataLoaders ---
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)
    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)
    
    return train_loader, val_loader, test_loader
'''
import torch
print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
print("CUDA version:", torch.version.cuda)

if torch.cuda.is_available():
    print("GPU Name:", torch.cuda.get_device_name(0))
else:
    print("‚ùå ƒêang ch·∫°y b·∫±ng CPU! Ki·ªÉm tra l·∫°i Driver.")
'''


train_loader, val_loader, test_loader = create_dataloaders(
    data_config=load_config("config/data_config.yml"), 
    batch_size=32
)

print(f"üöÄ DataLoaders created: Train batches={len(train_loader)}, Val batches={len(val_loader)}, Test batches={len(test_loader)}")

